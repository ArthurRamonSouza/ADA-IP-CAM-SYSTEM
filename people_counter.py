# python3 people_counter.py -p ./model/deploy.prototxt -m ./model/mobilenet_iter_73000.caffemodel -o ./output/output_01.avi

from core.centroid_tracker import CentroidTracker
from core.trackable_object import TrackableObject
from core.ai_model_setter import ModelSetter

import cv2
import imutils
from imutils.video import VideoStream
from imutils.video import FPS

import time
import dlib
import argparse

import numpy as np

# Constructing the argument parse and parsing the acceptable arguments
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--prototxt", required=True,
help="path to Caffe 'deploy' prototxt file")

ap.add_argument("-m", "--model", required=True,
help="path to Caffe pre-trained model")

ap.add_argument("-i", "--input", type=str,
help="path to optional input video file")

ap.add_argument("-o", "--output", type=str,
help="path to optional output video file")

ap.add_argument("-c", "--confidence", type=float, default=0.1,
help="minimum probability to filter weak detections")

ap.add_argument("-s", "--skip-frames", type=int, default=30,
help="# of skip frames between detections")

args = vars(ap.parse_args())

# To delete Print's after
print(f"Prototxt Path: {args['prototxt']}")
print(f"Model Path: {args['model']}")
print(f"Input Video Path: {args['input']}")
print(f"Output Video Path: {args['output']}")
print(f"Confidence Threshold: {args['confidence']}")
print(f"Skip Frames: {args['skip_frames']}")

# Model and prototxt path
model_path = "./model/mobilenet_iter_73000.caffemodel"
prototxt_path = "./model/deploy.prototxt"

# model_path = "./model/MobileNetSSD_deploy.caffemodel"
# prototxt_path = "./model/MobileNetSSD_deploy.prototxt"

# model_path = "./model/SSD_MobileNet.caffemodel"
# prototxt_path = "./model/SSD_MobileNet_prototxt.txt"


# model_path = "./model/MobileNetSSD_deploy_2.caffemodel"
# prototxt_path = "./model/MobileNetSSD_deploy_2.prototxt"

# Initing the model
print("[INFO] Loading the model...")
model = ModelSetter(model_path, prototxt_path)

# If a video path was not supplied, grab a reference to the webcam
if not args.get("input", False):
	print("[INFO] starting video stream...")
	vs = VideoStream(src=0).start()
	time.sleep(2.0)
	
# Otherwise, grab a reference to the video file
else:
	print("[INFO] opening video file...")
	vs = cv2.VideoCapture(args["input"])
	
# Initialize the video writer, if need, will be used later)
writer = None

# Frame dimensions (will be set as soon as we read the first frame from the video)
W = None
H = None
HALF_WIDITH = None

# Instantiate the centroid tracker, then initialize a list to store
# each of our dlib correlation trackers, followed by a dictionary to
# map each unique object ID to a TrackableObject
ct = CentroidTracker(max_disappeared=40, max_distance=50)
trackers = []
trackableObjects = {}

# Initialize the total number of frames processed thus far, along
# with the total number of objects that have moved either up or down
totalFrames = 0
entrances = 0
exits = 0

# Start the frames per second throughput estimator
fps = FPS().start()

def resize_with_padding(image, size):
    h, w = image.shape[:2]
    scale = min(size[1] / w, size[0] / h)
    nh, nw = int(h * scale), int(w * scale)
    resized = cv2.resize(image, (nw, nh))
    padded = cv2.copyMakeBorder(
        resized,
        (size[0] - nh) // 2,
        (size[0] - nh + 1) // 2,
        (size[1] - nw) // 2,
        (size[1] - nw + 1) // 2,
        cv2.BORDER_CONSTANT,
        value=(0, 0, 0)
    )
    return padded

# Loop over frames from the video stream
while True:
	
    # Picking the next frame handling if from
    # VideoCapture or VideoStream
    frame = vs.read()
    frame = frame[1] if args.get("input", False) else frame

    # If there are no frames left and we are in a video stream
    # the video is finished
    if args.get("input") is not False and frame is None:
        break

    frame = imutils.resize(frame, width=500)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # If the frame dimensions are empty, set them
    if W is None or H is None:
        (H, W) = frame.shape[:2]
        HALF_WIDITH = W // 2
        
    # If we are supposed to be writing a video to disk, initialize
    # the writer
    if args["output"] is not None and writer is None:
        fourcc = cv2.VideoWriter_fourcc(*"MJPG")
        writer = cv2.VideoWriter(args["output"], fourcc, 60, (W, H), True)
        
    # Status to out counter, it could be 'Waiting', 'Detecting' or 'Tracking'
    status = "Waiting"

    # To store the bouding boxes generated by the object detector or by the correlation trackers
    rects = []
    
    # The detection is done every N frames to maximize the performance
    if totalFrames % args.get("skip_frames") == 0:
        status = "Detecting"
        trackers = []

        # Converting the frame to a Binary Large OBject (blob) and send to the model
        # The values 127.5 and 0.007843 are chosen because they were used during
        # the training phase of models like MobileNet or SSD (Single Shot Detector)
        frame_resized = resize_with_padding(frame, (300, 300))
        blob = cv2.dnn.blobFromImage(frame_resized, 0.007843, (300, 300), 127.5)
        model.net.setInput(blob)

        # Propagates the input data through all the layers of the network
        detections = model.net.forward()

        # detections.shape[2] is he number of detected objects
        # Dimension 0 (1): Batch size (usually 1 for inference)
        # Dimension 1 (1): Indicates that the network produces only one set of detections.
        for p in np.arange(0, detections.shape[2]):

            # Dimension 3: The detection data for each object, often containing:
            # Index 0: Reserved (not used)
            # Index 1: Class ID or label of the detected object.
            # Index 2: Confidence score (likelihood of correct detection).
            # Index 3â€“6: Bounding box coordinates (x_min, y_min, x_max, y_max).
            confidence = detections[0, 0, p, 2]

            if confidence > args["confidence"]:
                idx = int(detections[0, 0, p, 1])
                
                if model.CLASSES[idx] != "person":
                    continue

                # In the detections tensor are usually in a normalized format
                box = detections[0, 0, p, 3:7] * np.array([W, H, W, H])
                (start_x, start_y, end_x, end_y) = box.astype("int")

                # Construct a dlib rectangle object from the coordinates
                # and then start the dlib correlation tracker
                tracker = dlib.correlation_tracker()
                rect = dlib.rectangle(start_x, start_y, end_x, end_y)
                cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), 1, 2)
                tracker.start_track(rgb, rect)

				# Add the tracker to the trackers list 
                trackers.append(tracker)

    # Only use the trackers objects to keep the people trackable
    else:

        for tracker in trackers:
            # Set system status to 'tracking'
            status = "Tracking"

            # Update the tracker and grab the updated position
            tracker.update(rgb)
            pos = tracker.get_position()

            # Unpack the position object
            start_x = int(pos.left())
            start_y = int(pos.top())
            end_x = int(pos.right())
            end_y = int(pos.bottom())

            # Add the bounding box coordinates to the rectangles list
            rects.append((start_x, start_y, end_x, end_y))
            cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), 1, 2)
    
    # Drawnig the line and the counters
    cv2.line(frame, (HALF_WIDITH, 0), (HALF_WIDITH, H), (0, 0, 255), 2)

    cv2.putText(frame, f"Entradas: {entrances}", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
    cv2.putText(frame, f"Saidas: {exits}", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

    # Centroid tracker to associate the old object
    # centroids with the newly computed object centroids
    objects = ct.update(rects)

    # Iterate for the trackeable objects
    for (objectID, centroid) in objects.items():

        # Finding the trackable objects by they IDs
        to = trackableObjects.get(objectID, None)

        # If there isn't, create a new object
        if to is None:
            to = TrackableObject(objectID, centroid)

        else:

            # Checking the direction calculating the subtraction of the current position and 
            # the mean of the past
            last_x = to.centroids[-1][0]
            to.centroids.append(centroid)

            if not to.counted:
                # Mooving to the right
                if last_x < HALF_WIDITH <= centroid[0]:
                    entrances+=1
                    to.counted = True
                
                # Mooving to the left
                elif last_x >= HALF_WIDITH > centroid[0]:
                    exits+=1
                    to.counted = True

		# Store the trackable object in the dictionary
        trackableObjects[objectID] = to
    
        # draw both the ID of the object and the centroid of the
        # object on the output frame
        text = "ID {}".format(objectID)
        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)

	# construct a tuple of information we will be displaying on the
	# frame
    info = [
          ("Entrances", entrances),
          ("Exits", exits),
          ("Status", status),
          ]

	# loop over the info tuples and draw them on our frame
    for (i, (k, v)) in enumerate(info):
        text = "{}: {}".format(k, v)
        cv2.putText(frame, text, (10, H - ((i * 20) + 20)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
    # check to see if we should write the frame to disk
    if writer is not None:
        writer.write(frame)
    # show the output frame
    cv2.imshow("Frame", frame)
    key = cv2.waitKey(1) & 0xFF
    # if the `q` key was pressed, break from the loop
    if key == ord("q"):
        break
    # increment the total number of frames processed thus far and
    # then update the FPS counter
    totalFrames += 1
    fps.update()

# stop the timer and display FPS information
fps.stop()
print("[INFO] elapsed time: {:.2f}".format(fps.elapsed()))
print("[INFO] approx. FPS: {:.2f}".format(fps.fps()))
# check to see if we need to release the video writer pointer
if writer is not None:
	writer.release()
# if we are not using a video file, stop the camera video stream
if not args.get("input", False):
	vs.stop()
# otherwise, release the video file pointer
else:
	vs.release()
# close any open windows
