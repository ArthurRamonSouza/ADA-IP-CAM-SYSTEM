# python3 people_counter.py -p ./model/deploy.prototxt -m ./model/mobilenet_iter_73000.caffemodel -o ./output/output_01.avi

from core.centroid_tracker import CentroidTracker
from core.trackable_object import TrackableObject
from core.ai_model_setter import ModelSetter

import cv2
import imutils
from imutils.video import VideoStream
from imutils.video import FPS

import time
import dlib
import argparse

import numpy as np

# Constructing the argument parse and parsing the acceptable arguments
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--prototxt", required=True,
help="path to Caffe 'deploy' prototxt file")

ap.add_argument("-m", "--model", required=True,
help="path to Caffe pre-trained model")

ap.add_argument("-i", "--input", type=str,
help="path to optional input video file")

ap.add_argument("-o", "--output", type=str,
help="path to optional output video file")

ap.add_argument("-c", "--confidence", type=float, default=0.4,
help="minimum probability to filter weak detections")

ap.add_argument("-s", "--skip-frames", type=int, default=30,
help="# of skip frames between detections")

args = vars(ap.parse_args())

# To delete Print's after
print(f"Prototxt Path: {args['prototxt']}")
print(f"Model Path: {args['model']}")
print(f"Input Video Path: {args['input']}")
print(f"Output Video Path: {args['output']}")
print(f"Confidence Threshold: {args['confidence']}")
print(f"Skip Frames: {args['skip_frames']}")

# Model path
model_path = "./model/mobilenet_iter_73000.caffemodel"
prototxt_path = "./model/deploy.prototxt"

# Initing the model
print("[INFO] Loading the model...")
model = ModelSetter(model_path, prototxt_path)

# If a video path was not supplied, grab a reference to the webcam
if not args.get("input", False):
	print("[INFO] starting video stream...")
	vs = VideoStream(src=0).start()
	time.sleep(2.0)
	
# Otherwise, grab a reference to the video file
else:
	print("[INFO] opening video file...")
	vs = cv2.VideoCapture(args["input"])
	
# Initialize the video writer, if need, will be used later)
writer = None

# Frame dimensions (will be set as soon as we read the first frame from the video)
W = None
H = None

# Instantiate the centroid tracker, then initialize a list to store
# each of our dlib correlation trackers, followed by a dictionary to
# map each unique object ID to a TrackableObject
ct = CentroidTracker(max_disappeared=40, max_distance=50)
trackers = []
trackableObjects = {}

# Initialize the total number of frames processed thus far, along
# with the total number of objects that have moved either up or down
totalFrames = 0
totalDown = 0
totalUp = 0

# Start the frames per second throughput estimator
fps = FPS().start()

# Loop over frames from the video stream
while True:
	
    # Picking the next frame handling if from
    # VideoCapture or VideoStream
    frame = vs.read()
    frame = frame[1] if args.get("input", False) else frame

    # If there are no frames left and we are in a video stream
    # the video is finished
    if args.get("input") is not False and frame is None:
        break

    frame = imutils.resize(frame, width=500)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # If the frame dimensions are empty, set them
    if W is None or H is None:
        (H, W) = frame.shape[:2]
        
    # If we are supposed to be writing a video to disk, initialize
    # the writer
    if args["output"] is not None and writer is None:
        fourcc = cv2.VideoWriter_fourcc(*"MJPG")
        writer = cv2.VideoWriter(args["output"], fourcc, 30, (W, H), True)
        
    # Status to out counter, it could be 'Waiting', 'Detecting' or 'Tracking'
    status = "Waiting"
    # To store the bouding boxes generated by the object detector or by the correlation trackers
    rects = []
    
    # The detection is done every N frames to maximize the performance
    if totalFrames % args.get("skip_frames") == 0:
        status = "Detecting"
        trackers = []

        # Converting the frame to a Binary Large OBject (blob) and send to the model
        # The values 127.5 and 0.007843 are chosen because they were used during
        # the training phase of models like MobileNet or SSD (Single Shot Detector)
        blob = cv2.dnn.blobFromImage(frame, 0.007843, (W, H), 127.5)
        model.net.setInput(blob)
        # Propagates the input data through all the layers of the network
        detections = model.net.forward()